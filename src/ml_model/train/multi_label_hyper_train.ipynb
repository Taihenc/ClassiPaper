{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, hamming_loss, f1_score,\n",
    "    precision_score, recall_score, jaccard_score,\n",
    "    label_ranking_average_precision_score, label_ranking_loss,\n",
    "    coverage_error, accuracy_score\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "projcect_root = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if projcect_root not in sys.path:\n",
    "    sys.path.append(projcect_root)\n",
    "from data_engineering.data import GetCleanedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    logging.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "\n",
    "def setup_logging(log_file: str):\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(\"Logging is set up.\")\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    logging.info(\"Starting data preprocessing.\")\n",
    "\n",
    "    df['clean_title'] = df['title']\n",
    "    df['clean_abstract'] = df['abstract']\n",
    "    df['clean_keywords'] = df['keywords']\n",
    "\n",
    "    df['combined_text'] = df['clean_title'].astype(str) + ' ' + df['clean_abstract'].astype(str) + ' ' + df['clean_keywords'].astype(str)\n",
    "\n",
    "    df = df[df['combined_text'].str.strip() != '']\n",
    "\n",
    "    before_filter = len(df)\n",
    "    class_counts = pd.Series([classes for classes in df['subject_area']]).value_counts()\n",
    "    valid_classes = class_counts[class_counts >= min_samples].index.tolist()\n",
    "    # filter only subject_area that are in valid_classes\n",
    "    df = df[df['subject_area'].apply(lambda x: x in valid_classes)]\n",
    "    logging.info(f\"Removed {before_filter - len(df)} samples with no valid labels after filtering.\")\n",
    "\n",
    "\n",
    "    class_counts = pd.Series([classes for classes in df['subject_area']]).value_counts()\n",
    "    logging.info(f\"Class Distribution After Handling Imbalance:\\n{class_counts}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_labels(df: pd.DataFrame) -> Tuple[np.ndarray, MultiLabelBinarizer]:\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(df['subject_area'])\n",
    "    logging.info(f\"Number of classes: {len(mlb.classes_)}\")\n",
    "    return y, mlb\n",
    "\n",
    "\n",
    "def split_data_iterative_stratification(X: pd.Series, y: np.ndarray, test_size: float, random_state: int):\n",
    "    logging.info(\"Splitting data using iterative stratification.\")\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_index, test_index in msss.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    logging.info(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def initialize_embedding_generator(model_name: str, device: str = None) -> Tuple[AutoTokenizer, AutoModel, str]:\n",
    "    logging.info(f\"Loading tokenizer and model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logging.info(f\"Model loaded on device: {device}\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "def get_cls_embeddings(text_list: List[str], tokenizer: AutoTokenizer, model: AutoModel, device: str, batch_size: int = 16) -> np.ndarray:\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(text_list), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch_text = text_list[i:i+batch_size]\n",
    "            encoded_input = tokenizer.batch_encode_plus(\n",
    "                batch_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            outputs = model(**encoded_input)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def find_optimal_thresholds(y_true, y_scores) -> np.ndarray:\n",
    "    n_classes = y_true.shape[1]\n",
    "    thresholds = np.zeros(n_classes)\n",
    "    for i in range(n_classes):\n",
    "        best_thr = 0.5\n",
    "        best_f1 = 0.0\n",
    "        for thr in np.linspace(0, 1, 101):\n",
    "            y_pred_label = (y_scores[:, i] >= thr).astype(int)\n",
    "            score = f1_score(y_true[:, i], y_pred_label, zero_division=0)\n",
    "            if score > best_f1:\n",
    "                best_f1 = score\n",
    "                best_thr = thr\n",
    "        thresholds[i] = best_thr\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def apply_thresholds(y_scores, thresholds) -> np.ndarray:\n",
    "    return (y_scores >= thresholds).astype(int)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_scores, target_names: list) -> dict:\n",
    "    logging.info(\"Evaluating the model.\")\n",
    "    \n",
    "    # Existing Metrics\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)\n",
    "    hl = hamming_loss(y_true, y_pred)\n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "    \n",
    "    # New Multi-Label Metrics\n",
    "    lr_ap = label_ranking_average_precision_score(y_true, y_scores)\n",
    "    lr_loss = label_ranking_loss(y_true, y_scores)\n",
    "    cov_error = coverage_error(y_true, y_scores)\n",
    "    subset_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'classification_report': report,\n",
    "        'hamming_loss': hl,\n",
    "        'micro_precision': micro_precision,\n",
    "        'micro_recall': micro_recall,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'jaccard_score': jaccard,\n",
    "        'label_ranking_average_precision_score': lr_ap,\n",
    "        'label_ranking_loss': lr_loss,\n",
    "        'coverage_error': cov_error,\n",
    "        'subset_accuracy': subset_acc\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_evaluation_metrics(metrics):\n",
    "    print(\"Evaluation metrics:\")\n",
    "    print(f\"Micro Precision: {metrics['micro_precision']}\")\n",
    "    print(f\"Micro Recall: {metrics['micro_recall']}\")\n",
    "    print(f\"Micro F1 Score: {metrics['micro_f1']}\")\n",
    "    print(f\"Macro Precision: {metrics['macro_precision']}\")\n",
    "    print(f\"Macro Recall: {metrics['macro_recall']}\")\n",
    "    print(f\"Macro F1 Score: {metrics['macro_f1']}\")\n",
    "    print(f\"Hamming Loss: {metrics['hamming_loss']}\")\n",
    "    print(f\"Jaccard Score: {metrics['jaccard_score']}\")\n",
    "    print(f\"Label Ranking Average Precision Score: {metrics['label_ranking_average_precision_score']}\")\n",
    "    print(f\"Label Ranking Loss: {metrics['label_ranking_loss']}\")\n",
    "    print(f\"Coverage Error: {metrics['coverage_error']}\")\n",
    "    print(f\"Subset Accuracy: {metrics['subset_accuracy']}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "\n",
    "\n",
    "def save_configuration(config: dict, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    logging.info(f\"Configuration saved to {save_path}\")\n",
    "\n",
    "\n",
    "def save_preprocessed_data(df: pd.DataFrame, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    logging.info(f\"Preprocessed data saved to {save_path}\")\n",
    "\n",
    "\n",
    "def save_tokenizer_and_model(tokenizer: AutoTokenizer, model: AutoModel, save_dir: str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    logging.info(f\"Tokenizer and model saved to {save_dir}\")\n",
    "\n",
    "\n",
    "def save_thresholds(thresholds: np.ndarray, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(thresholds, f)\n",
    "    logging.info(f\"Thresholds saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Configuration\n",
    "###################################\n",
    "config = {\n",
    "    \"data_dirs\": [\"../data/2018\",\n",
    "                  \"../data/2019\",\n",
    "                    \"../data/2020\",\n",
    "                    \"../data/2021\",\n",
    "                    \"../data/2022\",\n",
    "                    \"../data/2023\",\n",
    "                  ],\n",
    "    \"log_file\": \"/logs/training.log\",\n",
    "    \"embedding_model\": \"allenai/scibert_scivocab_uncased\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"min_samples_per_class\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"model_save_path\": \"/models/LogisticRegression_hyper_01/2018_2023/multi_label_classifier.pkl\",\n",
    "    \"mlb_save_path\": \"/models/LogisticRegression_hyper_01/2018_2023/mlb.pkl\",\n",
    "    \"tokenizer_model_save_dir\": \"/models/LogisticRegression_hyper_01/2018_2023/tokenizer_model/\",\n",
    "    \"thresholds_save_path\": \"/models/LogisticRegression_hyper_01/2018_2023/thresholds.pkl\",\n",
    "    \"config_save_path\": \"/models/LogisticRegression_hyper_01/2018_2023/config.json\",\n",
    "    \"preprocessed_data_save_path\": \"/models/LogisticRegression_hyper_01/2018_2023/preprocessed_data.csv\",\n",
    "    \"metrics_save_path\" : \"/models/LogisticRegression_hyper_01/2018_2023/evaluation_metrics.json\"\n",
    "    \"best_params_save_path\" \"/models/LogisticRegression_hyper_01/2018_2023/best_params.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Setup Logging\n",
    "###################################\n",
    "setup_logging(config['log_file'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Configuration\n",
    "###################################\n",
    "save_configuration(config, config['config_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Set Seed\n",
    "###################################\n",
    "set_seed(config['random_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load Data\n",
    "###################################\n",
    "# Get it from the data engineering module\n",
    "df = GetCleanedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Preprocess Data\n",
    "###################################\n",
    "df = preprocess_dataframe(df, min_samples=config['min_samples_per_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Encode Labels\n",
    "###################################\n",
    "y, mlb = encode_labels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Split Data\n",
    "###################################\n",
    "X_train_text, X_test_text, y_train, y_test = split_data_iterative_stratification(\n",
    "    df['combined_text'], y, test_size=config['test_size'], random_state=config['random_state']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split training set for validation (threshold tuning)\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_train_text, y_train, test_size=0.1, random_state=config['random_state']\n",
    ")\n",
    "logging.info(f\"After validation split - Training samples: {len(X_train_text)}, Validation samples: {len(X_val_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Generate Embeddings\n",
    "###################################\n",
    "tokenizer, model, device = initialize_embedding_generator(config['embedding_model'])\n",
    "X_train_embeddings = get_cls_embeddings(X_train_text.tolist(), tokenizer, model, device, batch_size=config['batch_size'])\n",
    "X_val_embeddings = get_cls_embeddings(X_val_text.tolist(), tokenizer, model, device, batch_size=config['batch_size'])\n",
    "X_test_embeddings = get_cls_embeddings(X_test_text.tolist(), tokenizer, model, device, batch_size=config['batch_size'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Tokenizer and Embedding Model\n",
    "###################################\n",
    "save_tokenizer_and_model(tokenizer, model, config['tokenizer_model_save_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a Pipeline with Fixed Hyperparameters\n",
    "###################################\n",
    "# Define fixed hyperparameters\n",
    "pca_n_components = 256 \n",
    "logistic_regression_C = 0.01 \n",
    "logistic_regression_class_weight = 'balanced'\n",
    "\n",
    "base_lr = LogisticRegression(\n",
    "    solver='saga',\n",
    "    C=logistic_regression_C,\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    "    random_state=config['random_state'],\n",
    "    class_weight=logistic_regression_class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "ovr = OneVsRestClassifier(base_lr)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca_n_components)),\n",
    "    ('classifier', ovr)\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': [128, 256, 512],\n",
    "    'classifier__estimator__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__estimator__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Train the model with GridSearchCV\n",
    "logging.info(\"Starting model training with GridSearchCV.\")\n",
    "grid_search.fit(X_train_embeddings, y_train)\n",
    "logging.info(\"Model training with GridSearchCV completed.\")\n",
    "\n",
    "# Replace the original pipeline with the best estimator\n",
    "pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the best parameters from GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params_save_path = \"../models/LogisticRegression_03/2018_2019//best_params.json\"\n",
    "os.makedirs(os.path.dirname(best_params_save_path), exist_ok=True)\n",
    "with open(best_params_save_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "logging.info(f\"Best parameters saved to {best_params_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params_save_path = \"../models/LogisticRegression_03/2018_2019//best_params.json\"\n",
    "os.makedirs(os.path.dirname(best_params_save_path), exist_ok=True)\n",
    "with open(best_params_save_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "logging.info(f\"Best parameters saved to {best_params_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Threshold Tuning on Validation Set\n",
    "###################################\n",
    "logging.info(\"Starting threshold tuning on validation set.\")\n",
    "y_val_scores = []\n",
    "scaler = pipeline.named_steps['scaler']\n",
    "pca = pipeline.named_steps['pca']\n",
    "classifier = pipeline.named_steps['classifier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation data\n",
    "val_features = scaler.transform(X_val_embeddings)\n",
    "val_features = pca.transform(val_features)\n",
    "\n",
    "# Get scores from each estimator\n",
    "for estimator in classifier.estimators_:\n",
    "    y_val_scores.append(estimator.predict_proba(val_features)[:, 1])\n",
    "y_val_scores = np.array(y_val_scores).T\n",
    "\n",
    "thresholds = find_optimal_thresholds(y_val, y_val_scores)\n",
    "logging.info(f\"Optimal thresholds per class: {thresholds}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Thresholds\n",
    "###################################\n",
    "save_thresholds(thresholds, config['thresholds_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Predict on Test Set\n",
    "###################################\n",
    "logging.info(\"Generating predictions on test set.\")\n",
    "y_test_scores = []\n",
    "test_features = scaler.transform(X_test_embeddings)\n",
    "test_features = pca.transform(test_features)\n",
    "for estimator in classifier.estimators_:\n",
    "    y_test_scores.append(estimator.predict_proba(test_features)[:, 1])\n",
    "y_test_scores = np.array(y_test_scores).T\n",
    "\n",
    "y_pred = apply_thresholds(y_test_scores, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Evaluate\n",
    "###################################\n",
    "metrics = evaluate_model(y_test, y_pred, y_test_scores, target_names=mlb.classes_)\n",
    "print_evaluation_metrics(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Model and Label Binarizer\n",
    "###################################\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(pipeline, config['model_save_path'])\n",
    "joblib.dump(mlb, config['mlb_save_path'])\n",
    "logging.info(\"Pipeline and label binarizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Evaluation Metrics\n",
    "###################################\n",
    "\n",
    "os.makedirs(os.path.dirname(config['metrics_save_path']), exist_ok=True)\n",
    "with open(config['metrics_save_path'], 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "logging.info(f\"Evaluation metrics saved to {config['metrics_save_path']}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClassiPaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
