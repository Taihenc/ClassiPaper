{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    logging.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "def setup_logging(log_file: str):\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(\"Logging is set up.\")\n",
    "\n",
    "def load_file(filename: str) -> Optional[dict]:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"JSONDecodeError for file {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error loading file {filename}: {e}\")\n",
    "    return None\n",
    "\n",
    "def load_files_from_dir(directory: str, executor: concurrent.futures.ThreadPoolExecutor) -> List[dict]:\n",
    "    if not os.path.isdir(directory):\n",
    "        logging.warning(f\"Directory does not exist: {directory}\")\n",
    "        return []\n",
    "    filenames = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if os.path.isfile(os.path.join(directory, f)) and (f.lower().endswith('.json') or '.' not in f)\n",
    "    ]\n",
    "    if not filenames:\n",
    "        logging.warning(f\"No JSON files found in directory: {directory}\")\n",
    "        return []\n",
    "\n",
    "    future_to_file = {executor.submit(load_file, filename): filename for filename in filenames}\n",
    "    loaded_data = []\n",
    "    for future in concurrent.futures.as_completed(future_to_file):\n",
    "        filename = future_to_file[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "            if data is not None:\n",
    "                loaded_data.append(data)\n",
    "            else:\n",
    "                logging.warning(f\"Data is None for file: {filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "    return loaded_data\n",
    "\n",
    "def load_data(data_dirs: List[str]) -> List[dict]:\n",
    "    logging.info(\"Starting data loading process.\")\n",
    "    all_data = []\n",
    "    num_workers = min(32, (multiprocessing.cpu_count() or 1) + 4)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(load_files_from_dir, directory, executor): directory\n",
    "            for directory in data_dirs\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            directory = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                all_data.extend(data)\n",
    "                logging.info(f\"Total data loaded so far: {len(all_data)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading data from directory {directory}: {e}\")\n",
    "\n",
    "    logging.info(f\"Completed data loading. Total records loaded: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "def extract_fields(data: List[dict]) -> pd.DataFrame:\n",
    "    logging.info(\"Extracting fields from data.\")\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    keywords = []\n",
    "    subject_areas = []\n",
    "\n",
    "    for item in data:\n",
    "        coredata = item.get('abstracts-retrieval-response', {}).get('coredata', {})\n",
    "        titles.append(coredata.get('dc:title', ''))\n",
    "        abstracts.append(coredata.get('dc:description', ''))\n",
    "\n",
    "        authkeywords = item.get('abstracts-retrieval-response', {}).get('authkeywords', {})\n",
    "        if isinstance(authkeywords, dict):\n",
    "            ak = authkeywords.get('author-keyword', [])\n",
    "            if isinstance(ak, list):\n",
    "                kws = [kw.get('$', '') for kw in ak]\n",
    "            elif isinstance(ak, dict):\n",
    "                kws = [ak.get('$', '')]\n",
    "            else:\n",
    "                kws = []\n",
    "        else:\n",
    "            kws = []\n",
    "        keywords.append(kws)\n",
    "\n",
    "        sa_list = item.get('abstracts-retrieval-response', {}).get('subject-areas', {}).get('subject-area', [])\n",
    "        subject_areas.append([area.get('$', '') for area in sa_list])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'title': titles,\n",
    "        'abstract': abstracts,\n",
    "        'keywords': keywords,\n",
    "        'subject_areas': subject_areas\n",
    "    })\n",
    "    logging.info(f\"Extracted fields into DataFrame with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def clean_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    logging.info(\"Starting data preprocessing.\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['clean_title'] = df['title'].apply(lambda x: clean_text(x, lemmatizer, stop_words))\n",
    "    df['clean_abstract'] = df['abstract'].apply(lambda x: clean_text(x, lemmatizer, stop_words))\n",
    "    df['clean_keywords'] = df['keywords'].apply(\n",
    "        lambda kws: clean_text(' '.join(kws), lemmatizer, stop_words) if isinstance(kws, list) else ''\n",
    "    )\n",
    "\n",
    "    df['combined_text'] = df['clean_title'].astype(str) + ' ' + df['clean_abstract'].astype(str) + ' ' + df['clean_keywords'].astype(str)\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df = df[df['combined_text'].str.strip() != '']\n",
    "    df = df[df['subject_areas'].map(lambda d: len(d)) > 0]\n",
    "    logging.info(f\"Dropped {initial_count - len(df)} rows due to empty text or missing subject areas.\")\n",
    "\n",
    "    # Filter to keep only subject areas ending with \"(all)\"\n",
    "    df['subject_areas'] = df['subject_areas'].apply(lambda x: [cls for cls in x if cls.endswith(\"(all)\")])\n",
    "\n",
    "    before_filter = len(df)\n",
    "    df = df[df['subject_areas'].map(lambda d: len(d)) > 0]\n",
    "    logging.info(f\"Removed {before_filter - len(df)} samples without subject areas ending with '(all)'.\")\n",
    "\n",
    "    # Now, handle multiple \"(all)\" subject areas by keeping only the most frequent one\n",
    "    # First, find the frequency of each \"(all)\" subject area\n",
    "    subject_counts = pd.Series([cls for classes in df['subject_areas'] for cls in classes]).value_counts()\n",
    "    logging.info(f\"Subject counts:\\n{subject_counts}\")\n",
    "\n",
    "    # Define a function to keep the most frequent \"(all)\" subject area\n",
    "    def keep_most_frequent_subject(subjects):\n",
    "        if len(subjects) == 1:\n",
    "            return subjects[0]\n",
    "        else:\n",
    "            # Sort subjects based on their frequency\n",
    "            sorted_subjects = sorted(subjects, key=lambda x: subject_counts.get(x, 0), reverse=True)\n",
    "            return sorted_subjects[0]\n",
    "\n",
    "    # Apply the function to create a new 'subject_area' column\n",
    "    df['subject_area'] = df['subject_areas'].apply(keep_most_frequent_subject)\n",
    "\n",
    "    # Drop the old 'subject_areas' column\n",
    "    df = df.drop(columns=['subject_areas'])\n",
    "    logging.info(f\"DataFrame shape after processing subject areas: {df.shape}\")\n",
    "\n",
    "    # Handle class imbalance by removing classes with fewer than min_samples\n",
    "    class_counts = df['subject_area'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= min_samples].index.tolist()\n",
    "    logging.info(f\"Valid classes after applying min_samples_per_class={min_samples}:\\n{class_counts[class_counts >= min_samples]}\")\n",
    "\n",
    "    df = df[df['subject_area'].isin(valid_classes)]\n",
    "    logging.info(f\"Final DataFrame shape after filtering classes: {df.shape}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_labels(df: pd.DataFrame) -> Tuple[np.ndarray, LabelEncoder]:\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['subject_area'])\n",
    "    logging.info(f\"Number of classes: {len(le.classes_)}\")\n",
    "    return y, le\n",
    "\n",
    "def split_data(X: pd.Series, y: np.ndarray, test_size: float, random_state: int) -> Tuple:\n",
    "    logging.info(\"Splitting data into train and test sets.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    logging.info(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def initialize_embedding_generator(model_name: str, device: str = None) -> Tuple[AutoTokenizer, AutoModel, str]:\n",
    "    logging.info(f\"Loading tokenizer and model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logging.info(f\"Model loaded on device: {device}\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "def get_cls_embeddings(text_list: List[str], tokenizer: AutoTokenizer, model: AutoModel, device: str, batch_size: int = 32) -> np.ndarray:\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(text_list), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch_text = text_list[i:i+batch_size]\n",
    "            encoded_input = tokenizer.batch_encode_plus(\n",
    "                batch_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            outputs = model(**encoded_input)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, target_names: list) -> dict:\n",
    "    logging.info(\"Evaluating the model.\")\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'classification_report': report,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall,\n",
    "        'f1_weighted': f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_evaluation_metrics(metrics):\n",
    "    print(\"Evaluation metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"Weighted Precision: {metrics['precision_weighted']}\")\n",
    "    print(f\"Weighted Recall: {metrics['recall_weighted']}\")\n",
    "    print(f\"Weighted F1 Score: {metrics['f1_weighted']}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "\n",
    "def save_configuration(config: dict, save_path: str):\n",
    "    # Create a copy of config to avoid modifying the original\n",
    "    config_to_save = config.copy()\n",
    "    \n",
    "    # Remove 'param_dist' if present\n",
    "    if 'hyperparameter_tuning' in config_to_save:\n",
    "        config_to_save['hyperparameter_tuning'] = config_to_save['hyperparameter_tuning'].copy()\n",
    "        if 'param_dist' in config_to_save['hyperparameter_tuning']:\n",
    "            del config_to_save['hyperparameter_tuning']['param_dist']\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(config_to_save, f, indent=4)\n",
    "    logging.info(f\"Configuration saved to {save_path}\")\n",
    "\n",
    "def save_preprocessed_data(df: pd.DataFrame, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    logging.info(f\"Preprocessed data saved to {save_path}\")\n",
    "\n",
    "def save_tokenizer_and_model(tokenizer: AutoTokenizer, model: AutoModel, save_dir: str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    logging.info(f\"Tokenizer and model saved to {save_dir}\")\n",
    "\n",
    "def save_model(model, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    joblib.dump(model, save_path)\n",
    "    logging.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "def save_label_encoder(le: LabelEncoder, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    joblib.dump(le, save_path)\n",
    "    logging.info(f\"Label Encoder saved to {save_path}\")\n",
    "\n",
    "def save_evaluation_metrics(metrics: dict, save_path: str):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    logging.info(f\"Evaluation metrics saved to {save_path}\")\n",
    "\n",
    "def save_best_parameters(best_params: dict, save_path: str):\n",
    "    \"\"\"\n",
    "    Saves the best hyperparameters to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        best_params (dict): The best hyperparameters found by the search.\n",
    "        save_path (str): The file path where the best parameters will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(best_params, f, indent=4)\n",
    "        logging.info(f\"Best hyperparameters saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save best hyperparameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Configuration\n",
    "###################################\n",
    "config = {\n",
    "    \"data_dirs\": [\"../../data/2018\",\n",
    "                  \"../../data/2019\",\n",
    "                  \"../../data/2020\",\n",
    "                  \"../../data/2021\",\n",
    "                  \"../../data/2022\",\n",
    "                  \"../../data/2023\"],\n",
    "    \"log_file\": \"../logs/training.log\",\n",
    "    \"embedding_model\": \"allenai/scibert_scivocab_uncased\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"min_samples_per_class\": 50, \n",
    "    \"batch_size\": 16,\n",
    "    \"model_save_path\": \"/models/LogisticRegression_single_hyper_01/2018_2023/single_label_classifier.pkl\",\n",
    "    \"label_encoder_save_path\": \"/models/LogisticRegression_single_hyper_01/2018_2023/label_encoder.pkl\",\n",
    "    \"tokenizer_model_save_dir\": \"/models/LogisticRegression_single_hyper_01/2018_2023/tokenizer_model/\",\n",
    "    \"preprocessed_data_save_path\": \"/models/LogisticRegression_single_hyper_01/2018_2023/preprocessed_data.csv\",\n",
    "    \"metrics_save_path\" : \"/models/LogisticRegression_single_hyper_01/2018_2023/evaluation_metrics.json\",\n",
    "    \"config_save_path\": \"/models/LogisticRegression_single_hyper_01/2018_2023/config.json\",\n",
    "    \"best_params_save_path\": \"/models/LogisticRegression_single_hyper_01/2018_2023/best_params.json\",  \n",
    "    \"hyperparameter_tuning\": {\n",
    "        \"enabled\": True,\n",
    "        \"method\": \"random\",\n",
    "        \"param_grid\": {\n",
    "            'pca__n_components': [256, 512, 768],\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "        },\n",
    "        # For RandomizedSearchCV\n",
    "        \"param_dist\": {\n",
    "            'pca__n_components': [256, 512, 768],\n",
    "            'classifier__C': loguniform(1e-3, 1e3),\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "        },\n",
    "        \"cv\": 3,\n",
    "        \"scoring\": 'f1_weighted',\n",
    "        \"n_iter\": 10,  # Only for RandomizedSearchCV\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Setup Logging\n",
    "###################################\n",
    "setup_logging(config['log_file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Configuration\n",
    "###################################\n",
    "save_configuration(config, config['config_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Set Seed\n",
    "###################################\n",
    "set_seed(config['random_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load Data\n",
    "###################################\n",
    "raw_data = load_data(config['data_dirs'])\n",
    "df = extract_fields(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Preprocessed Data\n",
    "###################################\n",
    "save_preprocessed_data(df, config['preprocessed_data_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Preprocess Data\n",
    "###################################\n",
    "df = preprocess_dataframe(df, min_samples=config['min_samples_per_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Encode Labels\n",
    "###################################\n",
    "y, le = encode_labels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Split Data\n",
    "###################################\n",
    "X_train_text, X_test_text, y_train, y_test = split_data(\n",
    "    df['combined_text'], y, test_size=config['test_size'], random_state=config['random_state']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Generate Embeddings\n",
    "###################################\n",
    "tokenizer, model, device = initialize_embedding_generator(config['embedding_model'])\n",
    "X_train_embeddings = get_cls_embeddings(X_train_text.tolist(), tokenizer, model, device, batch_size=config['batch_size'])\n",
    "X_test_embeddings = get_cls_embeddings(X_test_text.tolist(), tokenizer, model, device, batch_size=config['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Tokenizer and Embedding Model\n",
    "###################################\n",
    "save_tokenizer_and_model(tokenizer, model, config['tokenizer_model_save_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a Pipeline\n",
    "###################################\n",
    "# Define initial hyperparameters\n",
    "pca_n_components = 512  \n",
    "logistic_regression_C = 0.1 \n",
    "logistic_regression_class_weight = 'balanced'\n",
    "\n",
    "# Initialize Logistic Regression for multi-class classification\n",
    "lr = LogisticRegression(\n",
    "    solver='saga',\n",
    "    C=logistic_regression_C,\n",
    "    max_iter=8000,\n",
    "    n_jobs=-1,\n",
    "    random_state=config['random_state'],\n",
    "    class_weight=logistic_regression_class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca_n_components)),\n",
    "    ('classifier', lr)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Hyperparameter Tuning\n",
    "###################################\n",
    "if config['hyperparameter_tuning']['enabled']:\n",
    "    method = config['hyperparameter_tuning']['method']\n",
    "    cv = config['hyperparameter_tuning']['cv']\n",
    "    scoring = config['hyperparameter_tuning']['scoring']\n",
    "    \n",
    "    if method == \"grid\":\n",
    "        param_grid = config['hyperparameter_tuning']['param_grid']\n",
    "        search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            verbose=2,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif method == \"random\":\n",
    "        param_dist = config['hyperparameter_tuning']['param_dist']\n",
    "        n_iter = config['hyperparameter_tuning']['n_iter']\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            verbose=2,\n",
    "            random_state=config['random_state'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid hyperparameter tuning method. Choose 'grid' or 'random'.\")\n",
    "\n",
    "    # Train the model with hyperparameter tuning\n",
    "    logging.info(f\"Starting hyperparameter tuning using {method} search.\")\n",
    "    search.fit(X_train_embeddings, y_train)\n",
    "    logging.info(\"Hyperparameter tuning completed.\")\n",
    "    logging.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logging.info(f\"Best cross-validation {scoring}: {search.best_score_}\")\n",
    "\n",
    "    # Replace pipeline with the best estimator\n",
    "    best_pipeline = search.best_estimator_\n",
    "    \n",
    "    # Save the best hyperparameters\n",
    "    save_best_parameters(search.best_params_, config['best_params_save_path'])\n",
    "    \n",
    "    # # Optionally, save the GridSearchCV or RandomizedSearchCV object\n",
    "    # search_save_path = \"../models/LogisticRegression_single_02/2018_2023/hyperparameter_search.pkl\"\n",
    "    # joblib.dump(search, search_save_path)\n",
    "    # logging.info(f\"Hyperparameter search object saved to {search_save_path}\")\n",
    "else:\n",
    "    best_pipeline = pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Train the Pipeline (If Not Tuned)\n",
    "###################################\n",
    "if not config['hyperparameter_tuning']['enabled']:\n",
    "    logging.info(\"Starting model training.\")\n",
    "    best_pipeline.fit(X_train_embeddings, y_train)\n",
    "    logging.info(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Predict on Test Set\n",
    "###################################\n",
    "logging.info(\"Generating predictions on test set.\")\n",
    "y_test_pred = best_pipeline.predict(X_test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Evaluate\n",
    "###################################\n",
    "metrics = evaluate_model(y_test, y_test_pred, target_names=le.classes_)\n",
    "print_evaluation_metrics(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Model and Label Encoder\n",
    "###################################\n",
    "save_model(best_pipeline, config['model_save_path'])\n",
    "save_label_encoder(le, config['label_encoder_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Evaluation Metrics\n",
    "###################################\n",
    "save_evaluation_metrics(metrics, config['metrics_save_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Save Best Hyperparameters\n",
    "###################################\n",
    "# The best parameters have already been saved in the Hyperparameter Tuning section.\n",
    "# If you prefer to have a separate saving step, you can uncomment the following lines:\n",
    "\n",
    "# best_params = search.best_params_\n",
    "# save_best_parameters(best_params, config['best_params_save_path'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
